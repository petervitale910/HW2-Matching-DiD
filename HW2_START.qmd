---
title: "ðŸŒ¬ï¸ðŸ—³ Assignment 2: Wind Turbines, Matching, and Difference-in-Differences"
subtitle: "Replicate causal inference identification strategies in Stokes (2015) "
author: "EDS 241 / ESM 244 (DUE: 2/4/26)"
date: "January 26, 2026"
editor: 
  markdown: 
    wrap: 72
bibliography: references.bib
---

### Assignment instructions

Working with classmates to troubleshoot code and concepts is encouraged.
If you collaborate, list collaborators at the top of your submission.

All written responses must be written independently (in your own words).

Keep your work readable: Use clear headings and label plot elements
thoughtfully.

Assignment submission (YOUR NAME): Peter Vitale

------------------------------------------------------------------------

### Introduction

In this assignment you will be doing political weather forecasting
except the â€œstormsâ€ we care about are electoral swings that might follow
local wind turbine development.

In Stokes (2015), the idea is that a policy with diffuse benefits
(cleaner electricity) can create concentrated local costs (turbines
nearby), and those local opponents may â€œsend a signalâ€ at the ballot box
(i.e., NIMBYISM). Your job is to use two statistical tools:

-   Matching: Can we create a more apples-to-apples comparison between
    precincts that did vs. did not end up near turbine proposals?
-   Fixed effects + Difference-in-Differences: Can we use repeated
    elections to estimate how within-precinct changes in turbine
    exposure relate to changes in incumbent vote share?

------------------------------------------------------------------------

### Learning goal: Replicate the matching and fixed effects analyses from study:

> Stokes (2015): *"Electoral Backlash against Climate Policy: A Natural
> Experiment on Retrospective Voting and Local Resistance to Public
> Policy*.

-   **Study:** [Stokes (2015) -
    Article](https://drive.google.com/file/d/1y2Okzjq2EA43AW5JzCvFS8ecLpeP6NKh/view?usp=sharing)
-   **Data source:**
    [Dataverse-Stokes2015](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SDUGCC)

::: callout
`NOTE:` Replication of study estimates will be approximate. An
alternative matching procedure and fixed effects estimation package are
utilized in this assignment for illustration purposes.
:::

------------------------------------------------------------------------

### Setup: Load libraries

0.  Load libraries (+ install if needed)

```{r}

library(tidyverse)
library(here)
library(janitor)
library(jtools)

library(gtsummary)
library(gt)

library(MatchIt) # matching
library(cobalt)  # balance + love plots

library(fixest) # fast fixed effects
library(scales) # plotting

```

------------------------------------------------------------------------

### Part 1: Study Background

#### **1A.** Dive into the details of the study design and evaluation plan

> Goal: Get familiar with the study setting, environmental issue, and
> policy under evaluation.

::: callout
`NOTE:` Read over study to inform your response to the assignment
questions. For this assignment we will skip-over sections that describe
the *Instrumental Variables* identification strategy. We will cover
instrumental variable designs weeks 6-7.
:::

**1A.Q1** Summarize the environmental policy issue, the outcome of
interest, and the intervention being evaluated. Be sure to include a
brief description of each of the following key elements of the study:
unit of analysis, outcome, treatment, comparison group):

*Response:* The issue covered in this paper is the costs of
environmental policies, specifically wind energy projects, and how they
can manifest into political action. This political action is very
spatially biased to the areas with wind turbines, and the effect
diminishes the farther one lives from turbines. The analysis was
undertaken using provincial electoral districts precincts (unit of
analysis), and showed that the governing party when turbines were put in
place saw a diminished vote in the following election. The treatment in
this experiment were the turbines being built. The comparison groups
were different precincts at different distances from turbines, as well
as electoral districts as a whole.

**1A.Q2** Why might turbine proposals be correlated with baseline
political preferences or rural areas? Provide 2 plausible mechanisms,
and explain why that creates confounding.

*Response:* Rural areas swing more conservative naturally, which already
shifts the baseline against natural energies. Another plausible
mechanism is that education is generally lower in rural areas, which may
lead to lack of awareness of benefits of natural energy in relation to
costs.

------------------------------------------------------------------------

#### **1B.** Break down the causal inference strategy and identify threats to identification:

**1B.Q1** What is the key identifying assumption for a fixed effects /
Difference-in-Difference design? Explain how this assumption when
satisfied provides evidence of causal effect:

*Response:* The key assumption of the fixed affects model is Parallel
Trends Assumption (or Common Trends Assumption). This assumes that, in
the absence of treatment, the average outcomes for both treatment and
control groups would have followed the same, parallel trajectory over
time. When satisfied, this assumption shows a distinct impact (or
lacktherof) of the treatment. It essentially confirms that the change we
are seeing is due to the treatment instead of inherent differences in
the populations.

**1B.Q2** What is the reason for using a fixed effects approach from a
causal inference perspective? Summarize within the context of study (in
your own words).

*Response:* From a causal inference perspective, a fixed effect model is
great for dealing with issues that have many factors integrated within
them. This is because you can account for the factors in your models. In
our example, the study uses the model to add effects of income,
education, and others.

**1B.Q3** What part of the SUTVA assumption is most likely violated in
the context of this study design (and why)?

*Response:* Spillover is most likely violated in the context of the
study do to people inherently moving around. For example someone may
live farther from a turbine, but work nearer to one spreading the
treatment effect to what should be a control as they may start to
disdain the turbine. Furthermore, the `lines` in this project aren't
drawn super concretely. By using proximity to a turbine the lines become
blurred and spillover becomes more common.

**1B.Q4** Why does spillover matter when estimating an unbiased
treatment effect?

*Response:* Spillover matters when comparing treatment to control
becasue it can either unnaturally raise the control effect or dull the
treatment effect. Both create bias and lead to an obfuscation of the
results.

**1B.Q5** How do the authors assess the risk of spillovers, and what
analytic choice do they make to attempt to mitigate the risk that
spillover biases the causal estimate?

*Response:* To examine how far the effect persists over space, I also
examined whether vote share declines occur in precincts near turbines.
Further, when estimating the effect for each group, the sample excludes
units less than 6 km away from the turbines as control, to eliminate
spillovers when estimating each groupâ€™s treatment effect.

The authors assessed the risk by comparing whether party voting declined
in precincts near to turbines, and set the control line 3 km further
than the treatment as to reduce spillover.

------------------------------------------------------------------------

### Part 2: Matching

------------------------------------------------------------------------

We will start by evaluating the 2007 survey (cross-sectional) data.
Treatment is defined by whether a precinct is near a turbine proposal
(within 3 km).

> Goal: Match precincts using pre-treatment covariates and then estimate
> the effect of proposed wind turbines on incumbent vote share.

#### **2A.** Load data for matching

1.  Read in data file `stokes15_survey2007.csv`
2.  Code `precinct_id` and `district_id` as factors
3.  Take a look at the data

```{r}

match_data <- read_csv(here('data', 'stokes15_survey2007.csv')) %>% 
    mutate(precinct_id = factor(precinct_id),
           district_id = factor(district_id))


    
```

**2A.Q1** Intuition check: **Why match?** Explain rationale for using
this method.

*Response:* The reason for matching is to reduce selection bias and
confounding variables, which is good in a dataset like this.

------------------------------------------------------------------------

#### **2B.** Check imbalance (before matching)

-   Create a covariate *balance table* comparing treated and control
    precincts
-   Treatment indicator: `proposed_turbine_3km`
-   Include pre-treatment covariates: `log_home_val_07`, `p_uni_degree`,
    `log_median_inc`, `log_pop_denc`
-   Use the `tbl_summary()` function from the `{gtsummary}` package.

```{r}

match_data %>%
    select('proposed_turbine_3km', 'log_home_val_07',
           'p_uni_degree', 'log_median_inc') %>% 
    tbl_summary(
        by = proposed_turbine_3km,
        statistic = c(list(all_continuous() ~ "{mean} ({sd})"))) %>% 
        modify_header(label ~ "**Covariate**") %>%
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Proposed Turbine**")
    

```

**2B.Q1** Summarize the table output: Which covariates look
balanced/imbalanced?

*Response:* The groups (proposed turbines within 3km) are the most
uneven with \~5.5 thousand observations as a 0 (no) and only 354 as 1
(yes)

**2B.Q2** Describe in your own words why these covariates might be
expected to confound the treatment estimate:

*Response (2-4 sentences):* These covariates may be expected to confound
the treatment estimate for a variety of reasons. Log home value, income,
and education are all signifiers that tie to both social and political
leanings. Furthermore, issues arise dealing with non-uniform deployal of
turbines in low income communities [@lindvall2023]. This can lead to a
major confounding in our treatment effect, so controlling for it will be
necessary.

------------------------------------------------------------------------

**2B.Q3** Intuition check: What type of data do you need to conduct a
matching analysis?

*Response:* For matching analysis one wants to have many observations of our control as well as many pre-treatment observations, both of which we have in our dataset. 

------------------------------------------------------------------------

### Conduct matching estimation using the {`MatchIt`} package:

ðŸ“œ [Documentation - MatchIt](https://kosukeimai.github.io/MatchIt/)

Learning goals:

-   Approximate the Mahalanobis matching method used in Stokes (2015)
-   Implement another common matching approach called
    `propensity score matching`

::: callout
`NOTE`: In the replication code associated with Stokes (2015) the
{`AER`} package is used for Mahalanobis matching. In this assignment we
use the {`MatchIt`} package. The results are comparable but will not be
exactly the same.
:::

------------------------------------------------------------------------

### 2C. Mahalanobis nearest-neighbor matching

-   Conduct Mahalanobis matching\
-   Use nearest-neighbor match without replacement using Mahalanobis
    distance
-   Use 1-to-1 matching (match one control unit to each treatment unit)
-   Extract the matched data using `match.data()`

```{r}
set.seed(2412026)

match_model <- matchit(
      proposed_turbine_3km ~  log_home_val_07 + p_uni_degree +
          log_median_inc + log_pop_denc,
  data = match_data, 
  method = "nearest",       # Nearest neighbor matching
  distance = "mahalanobis", # Mahalanobis distance
  ratio = 1,                # Match one control unit to one treatment unit (1:1 matching)
  replace = FALSE           # Control observations are not replaced
)

# Extract matched data
matched_data <- match.data(match_model)

```

```{r}
summary(match_model)
```

**2C.Q1** Using the `summary()` output: Which covariate had the largest
and smallest `Std. Mean Diff.` before matching. Next, compare
largest/smallest `Std. Mean Diff.` after matching.

*Response:* Before matching the smallest `Std. Mean Diff.` is `p_uni_degree` at -.05 and the largest is `log_home_val_07` at .11. This changes drastically when matched and actually becomes a -.0093. After treatment the largest `Std. Mean Diff.` is `log_median_inc` at .0002 and the smallest is `log_pop_denc`. But in general all the `Std. Mean Diff.`'s all become **tiny** which is great! 

------------------------------------------------------------------------

#### 2D. Create a "love plot" using `love.plot()` â¤ï¸

ðŸ“œ [Documentation - cobalt](https://ngreifer.github.io/cobalt/)

-   Plot mean differences for data before & after matching across all
    pre-treatment covariates
-   This is an effective way to evaluate how effective matching was at
    achieving balance.

------------------------------------------------------------------------

-   Make a love plot of standardized mean differences (SMDs) before vs
    after matching.
-   Include a threshold line at 0.1.
-   In love plot display `mean.diffs`

```{r}

new_names <- data.frame(
    old = c("log_home_val_07", "p_uni_degree", "log_median_inc", "log_pop_denc"),
    new = c("Home Value (log)", "Percent University Degree",
            "Median Income (log)", "Population Density (log)"))

# Love plot
love.plot(match_model, stats = "mean.diffs",
          thresholds = c(m = 0.1),
          var.names = new_names)

```

**2D.Q1** Interpret the love plot in your own words:

*Response:* After matching our Covariate standard mean differences all become very close to 0. 

------------------------------------------------------------------------

### Propensity score matching

------------------------------------------------------------------------

#### 2E. Propensity Score Matching (PSM)

-   Estimate 1:1 nearest-neighbor Propensity Score Matching
-   Same code as above except change `distance = "logit"`

```{r}

set.seed(2412026)

propensity_scores <- matchit(
      proposed_turbine_3km ~  log_home_val_07 + p_uni_degree +
          log_median_inc + log_pop_denc,
  data = match_data, 
  method = "nearest",       # Nearest neighbor matching
  distance = "logit", # logit distance
  ratio = 1,                # Match one control unit to one treatment unit (1:1 matching)
  replace = FALSE           # Control observations are not replaced
)

# Extract matched data
matched_propensity_data <- match.data(propensity_scores)
    
```

------------------------------------------------------------------------

#### Create table displaying covariate balance using `cobalt::bal.tab()`

ðŸ“œ [Documentation - cobalt](https://ngreifer.github.io/cobalt/)

Use `bal.tab()` to report balance before and after matching.

```{r}

bal.tab(propensity_scores, 
        var.names = new_names) 

```
```{r}
bal.tab(match_model,
        var.names = new_names)
```

**2E.Q1** Compare Mahalanobis vs propensity score matching. Which method
did a better job at achieving balance?

*Response:* The Mahalanobis method seemed to do a better job at achieving balance, as the adjusted differences between groups is smaller (closer to zero).Both methods produced 354 matches. 

------------------------------------------------------------------------

#### 2F. Estimate an effect in the matched sample

Using the matched data (Mahalanobis method), estimate the effect of
treatment on the change in incumbent vote share (`change_liberal`).

```{r}

reg_match <- lm(change_liberal ~ proposed_turbine_3km, matched_data)

summ(reg_match, model.fit = FALSE)
```

**2F.Q1** Have you identified a causal estimate using this approach: Why
or why not?

*Response:* It seems we have identified a causal estimate, as the low p-value suggests the relationship between proposed turbine development and political change is (statistically significantly) unlikely to be random. This was done after the matching, which adds rigor to our estimate. 

**2F.Q2** When using a matching method, what is the main threat to
causal identification?

*Response:* The matching method can only match the covariates you give it, therefore omitted variable bias is still posible and remains a threat to causal identification.

**2F.Q3** Describe why the treatment estimate represents the
`Average Treatment for the Treated (ATT)` and explain why this is the
case relative to estimation of the `Average Treatment Effect (ATE)`.

*Response:* ATT represents the treatment effect of those treated, which were the 354 treated units (matches). The reason that we do not use ATE is if treatment is randomly assigned, but in our case we use the treatment effect for the treated as it is not random or equal. 

------------------------------------------------------------------------

### Part 3: Panel Data, Fixed Effects, and Difference-in-Difference

**Data source:**
[Dataverse-Stokes2015](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/SDUGCC)

------------------------------------------------------------------------

#### **3A:** Read in the panel data + code variables `precinct_id` and `year` as factors

```{r}

panel_data <-  read_csv(here('data', 'stokes15_panel_data.csv')) %>% 
    mutate(precinct_id = factor(precinct_id),
           year = factor(year))

# HINT: Try running `tabyl(panel_data$year)`. Review article to make sense of the row numbers (n).
tabyl(panel_data$year)
```

**3A.Q1:** Why are there 18,558 rows in `panel_data`?

*Response:* There are 18.5k rows because each precinct has a value over 3 years (6186 precincts * 3 years)

```{r}
# How many years are included in the panel?
print(paste0('There are ', n_distinct(panel_data$year), ' years included'))
# How many precincts are there?
print(paste0('There are ', n_distinct(panel_data$precinct_id), ' precincts included'))
```

**3A.Q2:** How many unique precincts are *ever treated* (i.e.,
`proposed` & `operational`)?


```{r}
print(paste0('There are ', n_distinct(panel_data %>% 
               filter(proposed_turbine == 1 |
                      operational_turbine == 1)), ' precincts that are ever treated'))
```


*Response:* See above

```{r}

panel_data %>%
  group_by(precinct_id) %>%
  summarise(
    ever_proposed    = any(proposed_turbine == 1, na.rm = TRUE),
    ever_operational = any(operational_turbine == 1, na.rm = TRUE),
    .groups = "drop") %>%
  summarise(
    n_ever_proposed    = sum(ever_proposed),
    n_ever_operational = sum(ever_operational))

```

------------------------------------------------------------------------

#### **3B.** Plot and evaluate parallel trends: Replicate `Figure.2` (Stokes, 2015)

1.  Create indicators for whether each precinct is ever treated by 2011
    (`treat_p`, `treat_o`; separate indicator for proposals and
    operational turbines).
2.  Plot mean incumbent vote share by year for treated vs control
    precincts (with 95% CIs).
3.  Facet by turbine type (proposed & operational)

Step 1: Prepare data

```{r}

trends_data <- panel_data %>%
  group_by(precinct_id) %>%
  mutate(
    treat_p = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),  # ever proposed (in any year)
    treat_o = as.integer(any(operational_turbine == 1, na.rm = TRUE))) %>% # ever operational (in any year)
  ungroup() %>% 
  pivot_longer(c(treat_p, treat_o),
               names_to = "turbine_type", values_to = "treat") %>% 
  mutate(
      turbine_type = factor(turbine_type,
                            levels = c("treat_p", "treat_o"),
                            labels = c("Proposed turbines", "Operational turbines")),  
    status = if_else(treat == 1, "Treated", "Control"),
    year   = factor(year))

```

Step 2: Create trends plot

```{r}

pd <- position_dodge(width = 0.15)

trends_data %>%
  group_by(turbine_type, status, year) %>%
  summarise(
    mean = mean(perc_lib, na.rm = TRUE),
    n    = sum(!is.na(perc_lib)),
    se   = sd(perc_lib, na.rm = TRUE) / sqrt(n), 
    ci   = qt(.975, df = pmax(n - 1, 1)) * se,
    .groups = "drop") %>%
ggplot(aes(year, mean, color = status, group = status)) +
  geom_line(position = pd, linewidth = 1.2) +
  geom_point(position = pd, size = 2.6) +
  geom_errorbar(
    aes(ymin = mean - ci, ymax = mean + ci),
    position = pd, width = .12, linewidth = .7, color = "black") +
  facet_wrap(~ turbine_type, nrow = 1) +
  scale_color_manual(values = c(Control = "#0072B2", Treated = "#B22222")) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  coord_cartesian(ylim = c(.20, .57)) +
  labs(
    title = "Figure 2. Trends in the Governing Partyâ€™s Vote Share",
    x = "Election Year",
    y = "Liberal Party Vote Share",
    color = NULL) +
  theme_minimal(base_size = 14) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "bottom",
    strip.text = element_text(face = "bold"))

```

**3B.Q1:** Write a short paragraph assessing the parallel trends
assumption for each outcome.

*Response (4-6 sentences):* 
This plot shows some parallel trends in both proposed and operational turbines. This being said, it relies on very few points. This, to quote Annie, makes all time periods seem equal when they arenâ€™t. Elections, war, and general discontent with the controlling parties could all be present in those large yearly jumps. These few data points with large jumps can cloud the results and create some doubt about parallel trends. Furthermore the lack of pre-proposal data makes truly assessing trends based on this data impossible. 

------------------------------------------------------------------------

### Estimating Fixed Effects Models (DiD) for proposals

$$
\text{Y}_{it}
=  \alpha_0 +
\beta \cdot (\text{proposed_turbine}_{it})
+ \gamma_i
+ \delta_t
+ \varepsilon_{it}
$$

-   $Y_{it}$ is the vote share for the Liberal Party in precinct *i* in
    time *t*
-   $\beta$ is the treatment effect of a turbine being proposed within a
    precinct
-   $\gamma_i$ is the precinct fixed effect
-   $\delta_t$ is the year fixed effect

------------------------------------------------------------------------

### Example 1: Randomly sample 40 precincts

-   To illustrate the "dummy variable method" of estimating fixed
    effects using the the general `lm()` function we are going to
    randomly sample 40 precincts (20 "treated" precincts with proposed
    turbines).
-   If we attempted to use this approach with the full sample estimating
    all 6185 (n-1) precinct-level coefficients is impractical (it would
    take a long time).

```{r}
set.seed(40002026)

precinct_frame <- panel_data %>%
  group_by(precinct_id) %>%
  summarise(
    proposed_turbine_any = as.integer(any(proposed_turbine == 1, na.rm = TRUE)),
    .groups = "drop"
  )

ids_40 <- precinct_frame %>%
  group_by(proposed_turbine_any) %>%
  slice_sample(n = 20) %>%
  ungroup() %>%
  select(precinct_id)

sample_40_precincts <- panel_data %>%
  semi_join(ids_40, by = "precinct_id")

```

------------------------------------------------------------------------

#### **3C:** Estimate a fixed effects model using `lm()` with fixed effects added for `precinct` and `year` using the sample of 40 precincts just created.

```{r}
model1_ff <- lm(perc_lib ~ operational_turbine + year + precinct_id, data = sample_40_precincts)

summ(model1_ff , model.fit = FALSE)
```

```{r}
summ(model1_ff , model.fit = FALSE, digits = 3, 
     robust = TRUE)
```

**3C.Q1:** Intuition check: Is the *signal-to-noise* ratio for the
treatment estimate greater than *2-to-1*?

*Response:* The signal to noise ratio for the treatment is not greater than 2 to 1, however for many of the precincts it is much greater than 2 to 1. 

> HINT: Add the argument `digits = 3` to the `summ()` function above

**3C.Q2:** Re-run the `summ()` function using the *heteroscedasticiy
robust standard error adjustment* (`robust = TRUE`). Did the standard
error (S.E.) estimates change? Explain why.

*Response:* The S.E. estimates became generally larger, this is because we adjusted to heteroscedasticity instead of homoscedasticity, which allows the standard errors to vary. This way the function doesnt oversimplify our errors. 

**3C.Q3:** Compare results of the model above to the findings from the
fixed effects analysis in the Stokes (2015) study. Why might the results
be similar or different?

*Response:* Our results differ from the stokes study because we randomly generated this data, and that will inherently change our data from the observed. 

**3C.Q4:** In your own words, explain why it is advantageous from a
causal inference perspective to include year and precinct fixed effects.
Explain how between-level and within-level variance is relevant to the
problem of omitted variable bias (OVB).

*Response (2-4 sentences):*
Year and precinct are very apropriate fixed effects. The year can drastically change the inherent political landscape in a country -- look at 2008 vs 2016 in the United States. Furthermore precincts can show community trends in political affiliation, which may influence a treatment effect. By including these variables as fixed effects we reduce between level variance, leaving the within-level variance we are testing, reducing omitted variable bias. 

------------------------------------------------------------------------

#### **3D.** Now using the full sample, estimate the treatment effect of wind turbine proposals on incumbent vote share. Use `feols()` from the `{fixest}` package to estimate the fixed effects.

See vignette here: [fixest walkthrough](https://cran.r-project.org/web/packages/fixest/vignettes/fixest_walkthrough.html#11_Estimation)

```{r}

model2_ff <- feols(perc_lib ~ proposed_turbine  | year + precinct_id, 
                   data = panel_data)

summary(model2_ff, cluster = ~precinct_id)
```

**3D.Q1:** Interpret the model results and translate findings to be
clear to an audience that may not have a background in causal inference
(Econometrics) methods.

In panel data settings, why is clustering by precinct important (i.e.,
`cluster = ~precinct_id`) ?â€

*Response (4-6 sentences):*
The effect of a proposed turbines on liberal voting is a drop of ~ 4%.  This means precincts with proposed wind turbines saw their liberal vote share decrease by about 4 points compared to similar precincts without turbines. This number is very unlikely to be derived from random data, and thus is statistically significant. The reason that it is important to cluster by precinct is because - as stated earlier- precincts can share characteristics which create correlated errors over time. This can skew our results and increase the error (how wrong we are) within our model. 

------------------------------------------------------------------------

#### **3E.** Estimate the treatment effect of *operational wind turbines* on incumbent vote share. Use the same approach as the previous model.

```{r}

model3_ff <- feols(perc_lib ~ operational_turbine  | year + precinct_id, 
                   data = panel_data)

summary(model3_ff, cluster = ~precinct_id)
```

**3E.Q1:** Interpret the `model3_ff` results as clearly and
**concisely** as you can.

*Response:* The implementation of an operational turbine reduces the liberal percentage of a community by 9%. This expectation is statistically significant, meaning it is likely not due to random chance. 

**3E.Q2:** Why do you think the effect of proposed wind turbines is
different from operational wind turbines. Develop your own theory about
why incumbent vote share is affected in this way. Use the Stokes (2015)
study to inform your response as needed.

*Response:*  Proposed wind turbines are not realized, as in there is no direct effect to the proposal of a turbine. This may lead some to not even know it was proposed. Operational turbines, on the other hand, are hard to miss. The visibility, noise pollution, and construction issues make people more incensed to make a change in their voting behaviors. 

------------------------------------------------------------------------

```{r, message=TRUE, echo=FALSE, eval=FALSE}

library(praise); library(cowsay)

praise("${EXCLAMATION}! ðŸš€ Great work - You are ${adjective}! ðŸ’«")

say("The End", "duck")
```
